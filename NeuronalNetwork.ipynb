{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPFvdV/FA457Yb/8iOrhyJP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/javicorrales/transformers_rx/blob/main/NeuronalNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "A3gl6ao86cj-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        " \n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        " \n",
        "def sigmoid_derivada(x):\n",
        "    return sigmoid(x)*(1.0-sigmoid(x))\n",
        " \n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        " \n",
        "def tanh_derivada(x):\n",
        "    return 1.0 - x**2\n",
        " \n",
        " \n",
        "class NeuralNetwork:\n",
        " \n",
        "    def __init__(self, layers, activation='tanh'):\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = sigmoid\n",
        "            self.activation_prime = sigmoid_derivada\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = tanh\n",
        "            self.activation_prime = tanh_derivada\n",
        " \n",
        "        # inicializo los pesos\n",
        "        self.weights = []\n",
        "        self.deltas = []\n",
        "        # capas = [2,3,2]\n",
        "        # rando de pesos varia entre (-1,1)\n",
        "        # asigno valores aleatorios a capa de entrada y capa oculta\n",
        "        for i in range(1, len(layers) - 1):\n",
        "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
        "            self.weights.append(r)\n",
        "        # asigno aleatorios a capa de salida\n",
        "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
        "        self.weights.append(r)\n",
        " \n",
        "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
        "        # Agrego columna de unos a las entradas X\n",
        "        # Con esto agregamos la unidad de Bias a la capa de entrada\n",
        "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
        "        X = np.concatenate((ones.T, X), axis=1)\n",
        "        \n",
        "        for k in range(epochs):\n",
        "            i = np.random.randint(X.shape[0])\n",
        "            a = [X[i]]\n",
        " \n",
        "            for l in range(len(self.weights)):\n",
        "                    dot_value = np.dot(a[l], self.weights[l])\n",
        "                    activation = self.activation(dot_value)\n",
        "                    a.append(activation)\n",
        "            # Calculo la diferencia en la capa de salida y el valor obtenido\n",
        "            error = y[i] - a[-1]\n",
        "            deltas = [error * self.activation_prime(a[-1])]\n",
        "            \n",
        "            # Empezamos en el segundo layer hasta el ultimo\n",
        "            # (Una capa anterior a la de salida)\n",
        "            for l in range(len(a) - 2, 0, -1): \n",
        "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
        "            self.deltas.append(deltas)\n",
        " \n",
        "            # invertir\n",
        "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
        "            deltas.reverse()\n",
        " \n",
        "            # backpropagation\n",
        "            # 1. Multiplcar los delta de salida con las activaciones de entrada \n",
        "            #    para obtener el gradiente del peso.\n",
        "            # 2. actualizo el peso restandole un porcentaje del gradiente\n",
        "            for i in range(len(self.weights)):\n",
        "                layer = np.atleast_2d(a[i])\n",
        "                delta = np.atleast_2d(deltas[i])\n",
        "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
        " \n",
        "            if k % 10000 == 0: print('epochs:', k)\n",
        " \n",
        "    def predict(self, x): \n",
        "        ones = np.atleast_2d(np.ones(x.shape[0]))\n",
        "        a = np.concatenate((np.ones(1).T, np.array(x)), axis=0)\n",
        "        for l in range(0, len(self.weights)):\n",
        "            a = self.activation(np.dot(a, self.weights[l]))\n",
        "        return a\n",
        " \n",
        "    def print_weights(self):\n",
        "        print(\"LISTADO PESOS DE CONEXIONES\")\n",
        "        for i in range(len(self.weights)):\n",
        "            print(self.weights[i])\n",
        " \n",
        "    def get_deltas(self):\n",
        "        return self.deltas"
      ]
    }
  ]
}